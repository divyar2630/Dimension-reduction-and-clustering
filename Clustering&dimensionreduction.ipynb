{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFq17LI3eaQF"
   },
   "source": [
    "# Dimension Reduction and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7ZQ6ntAd8VZ"
   },
   "source": [
    "#### DS 862 - Project I\n",
    "#### Authors\n",
    "* Syed Asim\n",
    "* Divya Raghunathan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5iQh2zyd8Va"
   },
   "source": [
    "### Table of Contents\n",
    "\n",
    "\n",
    "* [I: Project Overview](#one)\n",
    "* [II: Introduction](#two)\n",
    "* [Part I : Dimension Reduction](#1)\n",
    "    * [1.1 Split the data into train, valid , train+valid and test set](#1.1)\n",
    "    * [1.2 Split the four datasets into numerical and categorical variables](#1.2)\n",
    "    * [1.3 Maintaining the levels of the categorical variables](#1.3)\n",
    "    * [1.4 Train and Valid set](#1.4)\n",
    "        * [1.4.1 Scaling the data- numerical train and valid](#1.4.1)\n",
    "        * [1.4.2 Setting hyperparameters](#1.4.2)\n",
    "        * [1.4.3 Perform PCA,MCA and ridge regression on the train set and evaluate on the valid set](#1.4.3)\n",
    "    * [1.5 Train+Valid and Test set](#1.5)\n",
    "        * [1.5.1 Rescaling the data - train+valid and test numerical set](#1.5.1)\n",
    "        * [1.5.2 Perform PCA, MCA and Ridge regression on the train+valid using the best parameters and evaluate on the test set](#1.5.2)\n",
    "    * [1.6 Ridge regression on the original dataset](#1.6)\n",
    "    * [1.7 Findings of Part I](#1.7)\n",
    "* [Part II : Clustering Analysis](#2)\n",
    "    * [2.1 Computer gower distance on the predictor set ](#2.1)\n",
    "    * [2.2 Apply K-medoids using the gower distance matrix as input ](#2.2)\n",
    "    * [2.3 Comparing clustering results ](#2.3)\n",
    "    * [2.4 Findings of Part II](#2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZ-SrLGgd8Vb"
   },
   "source": [
    "# **I: Project Overview** <a class=\"anchor\" id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g797O7Bid8Vc"
   },
   "source": [
    "The project is divided into two parts- Dimension reduction and Clustering. In dimension reduction, we apply PCA on the numerical features and MCA on the categorical features. We will tune PCA and MCA using a train/valid and test split. Finally, we will tune parameters & perform ridge regression on the reduced features and find the mean squared error. \n",
    "In clustering, we will use Gower distance on the mixed numerical and categorical dataset. We will apply k-mediods clustering method and obtain the clusters.\n",
    "For the above two methods, we will compare the results in the following way:\n",
    "\n",
    "\n",
    "1.   For dimension reduction, we will compare the results by performing ridge regression on the original dataset.\n",
    "2.   For clustering, we will compare the result with the ground truth.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjjXSYuAd8Vc"
   },
   "source": [
    "# **II: Introduction** <a class=\"anchor\" id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sT-kqWQud8Ve"
   },
   "source": [
    "In this notebook, we will be applying  dimension reduction techniques and a clustering method on the housing dataset. The housing dataset can be found here (https://drive.google.com/file/d/1F9Z03GYHppfBbmDFvNsA89i2OQm8N-4T/view?usp=sharing). The response variable on this dataset is the price of the house. The dataset contains 80 features (including the response variable) and 1460 observations. After removing columns with more than 30 null values, we are left with 64 features (including response). Out of the 63 predictor variables, 34 variables are numerical and 29 variables are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqPpSutiowOE"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Part 1 - Dimension Reduction imports\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "import prince\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "#Part 2- Clustering imports\n",
    "\n",
    "import gower\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P10w6n9mowOV",
    "outputId": "94fb8641-e746-4471-a346-a6c14346526a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset : (1460, 80)\n"
     ]
    }
   ],
   "source": [
    "# Load data set\n",
    "data = pd.read_csv('train.csv')\n",
    "data = data.drop('Id', axis = 1)\n",
    "print(\"Shape of the dataset : {}\".format(data.shape))\n",
    "\n",
    "# Remove columns that have too many missing values\n",
    "data = data.drop(data.columns[data.isnull().sum() > 30], axis = 1)\n",
    "\n",
    "# Remove missing values\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "fW9D14Qzd8Vz",
    "outputId": "58bd8f9f-db3b-4a1b-ae3a-ba40e4979d87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\n",
       "0          60       RL     8450   Pave      Reg         Lvl    AllPub   \n",
       "1          20       RL     9600   Pave      Reg         Lvl    AllPub   \n",
       "2          60       RL    11250   Pave      IR1         Lvl    AllPub   \n",
       "3          70       RL     9550   Pave      IR1         Lvl    AllPub   \n",
       "4          60       RL    14260   Pave      IR1         Lvl    AllPub   \n",
       "\n",
       "  LotConfig LandSlope Neighborhood  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "0    Inside       Gtl      CollgCr  ...             0         0           0   \n",
       "1       FR2       Gtl      Veenker  ...             0         0           0   \n",
       "2    Inside       Gtl      CollgCr  ...             0         0           0   \n",
       "3    Corner       Gtl      Crawfor  ...           272         0           0   \n",
       "4       FR2       Gtl      NoRidge  ...             0         0           0   \n",
       "\n",
       "  PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition SalePrice  \n",
       "0        0        0       2    2008        WD        Normal    208500  \n",
       "1        0        0       5    2007        WD        Normal    181500  \n",
       "2        0        0       9    2008        WD        Normal    223500  \n",
       "3        0        0       2    2006        WD       Abnorml    140000  \n",
       "4        0        0      12    2008        WD        Normal    250000  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BAEbjo7kowOi",
    "outputId": "73b24dbc-55bd-4a10-b7ee-94c7788057a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows and columns in the dataset : (1451, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of rows and columns in the dataset : {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDGuxvsWowOn"
   },
   "outputs": [],
   "source": [
    "# Defining the predictors and response variables\n",
    "X = data.drop('SalePrice', axis = 1)\n",
    "y = data.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "VoqfX07vd8WB",
    "outputId": "e5006368-0c3f-46be-b21a-3da4b2674fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of numerical variables in the dataset : 34\n",
      "The number of categorical variables in the dataset : 29\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of numerical variables in the dataset : {}\".format(X.select_dtypes(include=[np.number]).shape[1]))\n",
    "print(\"The number of categorical variables in the dataset : {}\".format(X.select_dtypes(exclude=[np.number]).shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycjcfuKjiTbc"
   },
   "source": [
    "# **Part I : Dimension Reduction** <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dQhZOwCibSn"
   },
   "source": [
    "To perform dimension reduction on our dataset, we will use Principle componenet analysis (PCA) on the numerical data and Multiple Correspondence Analysis (MCA) on the categorical variables. Prince library is used for MCA and PCA here, https://github.com/MaxHalford/prince. Step-by-step explanation of the code is as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85-Q-kPmj3wv"
   },
   "source": [
    "\n",
    "\n",
    "1.   Split the data into train,valid and test set(60%-20%-20% split)\n",
    "\n",
    "2.   Split the training and validation set into categorical and numerical features (2 sets will split into four)- X training numerical, X training catgorical, X validation numerical, X validation categorical \n",
    "\n",
    "3.  Preparing the categorical data for MCA. It is important that the training and validation data have the same number of levels and the same levels. Else the algorithm will throw a \"dimension mismatch\" error. To avoid the error we will remove any level that is not present in either of the two sets- training or validation. Now the data is prepped for MCA\n",
    "\n",
    "4.   Preparing the numerical data for PCA.Scaling- `Fit` the numerical training data, `Transform` the numerical training & validation data \n",
    " \n",
    "5.   Define the hyperparameter sets using `itertools`. We will tune 3 parameters here- Number of components - PCA, MCA and number of iteration \n",
    "\n",
    "6. For each combination of hyperparameters , `fit` PCA on numerical training data and `fit` MCA on the categorical training data. The result would be two numerical dataframes. Combine the two dataframes to one.\n",
    "\n",
    "7. Tune `alphas` for ridge regression and `Fit` the combined dataset.\n",
    "\n",
    "8. `Transform` PCA on the validation numerical and transofrm MCA on validation catgorical, combine the results.\n",
    "\n",
    "9. Predict using ridge regression for various alpha values and find the mean squared error of the validation set. We will get a best combination hyperparameter set in this step- Number of components- PCA, MCA, iterations andf alpha.\n",
    "\n",
    "10. Now combine training and validation set, let's call it train+valid set.\n",
    "\n",
    "11. Repeat steps 4,6,7,8 and 9, but this time we won't tune the PCA, MCA and ridge regression hyper parameters, we will directly use the best results from the above steps. Instead of the train set, we will use the train+valid set and then evaluate on the test set. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeNi22iRflDg"
   },
   "source": [
    "#### 1.1 Split the data into train, valid , train+valid and test set <a class=\"anchor\" id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCcv4i84uesy"
   },
   "outputs": [],
   "source": [
    "# Using 60%-20%-20% split.\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yh1XEvcfyE-"
   },
   "source": [
    "#### 1.2 Split the four datasets into numerical and categorical variables <a class=\"anchor\" id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ujk4NQOcukHN"
   },
   "outputs": [],
   "source": [
    "X_train_valid_num = X_train_valid.select_dtypes(include=[np.number])\n",
    "X_train_valid_cat = X_train_valid.select_dtypes(exclude=[np.number])\n",
    "\n",
    "X_test_num = X_test.select_dtypes(include=[np.number])\n",
    "X_test_cat = X_test.select_dtypes(exclude=[np.number])\n",
    "\n",
    "X_train_num = X_train.select_dtypes(include=[np.number])\n",
    "X_train_cat = X_train.select_dtypes(exclude=[np.number])\n",
    "\n",
    "X_valid_num = X_valid.select_dtypes(include=[np.number])\n",
    "X_valid_cat = X_valid.select_dtypes(exclude=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hupHNyQxiU4n"
   },
   "source": [
    "#### 1.3 Maintaining the levels of the categorical variables <a class=\"anchor\" id=\"1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6H2a2m0suyxD"
   },
   "outputs": [],
   "source": [
    "#Keeping the variables with the same number of levels in each of the categorical feature in train+valid set and test set\n",
    "keep = X_train_valid_cat.nunique() == X_test_cat.nunique() \n",
    "X_train_valid_cat = X_train_valid_cat[X_train_valid_cat.columns[keep]]\n",
    "X_test_cat = X_test_cat[X_test_cat.columns[keep]]\n",
    "X_train_cat = X_train_cat[X_train_cat.columns[keep]]\n",
    "X_valid_cat = X_valid_cat[X_valid_cat.columns[keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WcH7x9A-yTs"
   },
   "outputs": [],
   "source": [
    "#Keeping the variables with the same number of levels in each of the categorical feature in train set and valid set\n",
    "keep2 = X_train_cat.nunique() == X_valid_cat.nunique()\n",
    "X_train_valid_cat = X_train_valid_cat[X_train_valid_cat.columns[keep2]]\n",
    "X_test_cat = X_test_cat[X_test_cat.columns[keep2]]\n",
    "X_train_cat = X_train_cat[X_train_cat.columns[keep2]]\n",
    "X_valid_cat = X_valid_cat[X_valid_cat.columns[keep2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgj71a7vvdMX"
   },
   "outputs": [],
   "source": [
    "# For the categorical features that have same number of levels, we will keep the variables which have the same classes in the train+valid set and test set \n",
    "keep = []\n",
    "for i in range(X_train_valid_cat.shape[1]):\n",
    "    keep.append(all(np.sort(X_train_valid_cat.iloc[:,i].unique()) == np.sort(X_test_cat.iloc[:,i].unique())))\n",
    "X_train_valid_cat = X_train_valid_cat[X_train_valid_cat.columns[keep]]\n",
    "X_test_cat = X_test_cat[X_test_cat.columns[keep]]\n",
    "X_train_cat = X_train_cat[X_train_cat.columns[keep]]\n",
    "X_valid_cat = X_valid_cat[X_valid_cat.columns[keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vELDEqwLii3i"
   },
   "outputs": [],
   "source": [
    "# For the categorical features that have same number of levels, we will keep the variables which have the same classes in the train set and valid set\n",
    "keep2 = []\n",
    "for i in range(X_train_cat.shape[1]):\n",
    "    keep2.append(all(np.sort(X_train_cat.iloc[:,i].unique()) == np.sort(X_valid_cat.iloc[:,i].unique())))\n",
    "X_train_valid_cat = X_train_valid_cat[X_train_valid_cat.columns[keep2]]\n",
    "X_test_cat = X_test_cat[X_test_cat.columns[keep2]]\n",
    "X_train_cat = X_train_cat[X_train_cat.columns[keep2]]\n",
    "X_valid_cat = X_valid_cat[X_valid_cat.columns[keep2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "xaTGaeCXvlQI",
    "outputId": "7696ab3b-ac84-4f55-caeb-fedc5f446cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all categorical datasets: (1160, 10) (870, 10) (290, 10) (291, 10)\n",
      "Shape of all numerical datasets  : (1160, 34) (870, 34) (290, 34) (291, 34)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of all categorical datasets:', X_train_valid_cat.shape, X_train_cat.shape, X_valid_cat.shape, X_test_cat.shape)\n",
    "print('Shape of all numerical datasets  :', X_train_valid_num.shape, X_train_num.shape, X_valid_num.shape, X_test_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "houMRSNwjZVt"
   },
   "source": [
    "\n",
    "#### 1.4 Train and Valid set <a class=\"anchor\" id=\"1.4\"></a>\n",
    "\n",
    "##### 1.4.1 Scaling the data- numerical train and valid <a class=\"anchor\" id=\"1.4.1\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "enZQd4CbvzYE"
   },
   "outputs": [],
   "source": [
    "#scaling the numerical dat train and valid set\n",
    "scaler = StandardScaler() # Instantiate\n",
    "scaler.fit(X_train_num) # Fitting the data\n",
    "X_train_numeric_S = pd.DataFrame(scaler.transform(X_train_num)) # transforming the training data\n",
    "X_valid_numeric_S = pd.DataFrame(scaler.transform(X_valid_num)) # transforming the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kStQRsq-uNLB"
   },
   "source": [
    "##### 1.4.2 Setting hyperparameters <a class=\"anchor\" id=\"1.4.2\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQETsP2ad8Ww"
   },
   "source": [
    "We will use \"Number of components\" and \"Number of iterations\" as tuning parameter for PCA and MCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp7DDSCQz-C9"
   },
   "outputs": [],
   "source": [
    "comps_pca = np.arange(3,35) #Number of components for PCA\n",
    "comps_mca = np.arange(3,11) #Number of components for MCA\n",
    "iters = np.arange(1,10) #Number of iterations for PCA & MCA\n",
    "# tuning parameters for Regression\n",
    "alphas = np.logspace(-10,10,21) # lambda values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zMATT2zAz-La",
    "outputId": "547440b7-d0e4-4b4e-a9f3-88552b0d1c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hyperparameter sets in total: 48384\n"
     ]
    }
   ],
   "source": [
    " #Forming all possible combinations for number of components- PCA, number of components- MCA and number of iterations\n",
    "hyperparameter_set = list(itertools.product(comps_pca,comps_mca,iters,alphas))\n",
    "print(\"The number of hyperparameter sets in total: {}\".format(len(hyperparameter_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbg6iKy-vWz1"
   },
   "source": [
    "##### 1.4.3 Perform PCA,MCA and ridge regression on the train set and evaluate on the valid set<a class=\"anchor\" id=\"1.4.3\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxKjPPp28cuy"
   },
   "outputs": [],
   "source": [
    "MSE_valid = []\n",
    "comp_pca = []\n",
    "comp_mca = []\n",
    "iter = []\n",
    "alpha = []\n",
    "\n",
    "#For-loop for each of the hyperparameter set\n",
    "\n",
    "for val in hyperparameter_set:\n",
    "    \n",
    "#For each combination of hyperparameters, fit and transform PCA on numerical training data and fit and transform MCA on the categorical training data. \n",
    "\n",
    "    pca = prince.PCA(n_components=val[0], n_iter=val[2], random_state=42) \n",
    "    pca = pca.fit(X_train_numeric_S)\n",
    "    X_train_pca = pca.transform(X_train_numeric_S)\n",
    "    X_train_pca = X_train_pca.reset_index() #Resetting index to concat in a future step\n",
    "    \n",
    "    mca = prince.MCA(n_components=val[1], n_iter=val[2], random_state=42)\n",
    "    mca = mca.fit(X_train_cat)\n",
    "    X_train_mca = mca.transform(X_train_cat)\n",
    "    X_train_mca = X_train_mca.reset_index() #Resetting index to concat in a future step\n",
    "    \n",
    "#Combine the PCA and MCA results from above into one dataframe\n",
    "    X_train_pmca = pd.concat([X_train_pca, X_train_mca], axis=1) # X_train_pmca contains numerical values of the reduced features \n",
    "\n",
    "#Transform the valid-numerical and categorical data using PCA and MCA respectively\n",
    "    X_valid_pca = pca.transform(X_valid_numeric_S)\n",
    "    X_valid_pca = X_valid_pca.reset_index()\n",
    "    X_valid_mca = mca.transform(X_valid_cat)\n",
    "    X_valid_mca = X_valid_mca.reset_index()\n",
    "\n",
    "#Combine the PCA and MCA results from above into one dataframe    \n",
    "    X_valid_pmca = pd.concat([X_valid_pca,X_valid_mca], axis=1)\n",
    "    \n",
    "    #Appending the hyperparameters to a list for each run\n",
    "    alpha.append(val[3])\n",
    "    comp_pca.append(val[0])\n",
    "    comp_mca.append(val[1])\n",
    "    iter.append(val[2])\n",
    "    #Fit ridge regression on the reduced features for each alpha value\n",
    "    lm = linear_model.Ridge(alpha=val[3])\n",
    "    lm.fit(X_train_pmca,y_train)\n",
    "    \n",
    "    #Evaluate the results using mean squared error and store the results in MSE_valid  \n",
    "    MSE_valid.append(metrics.mean_squared_error(lm.predict(X_valid_pmca),y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "Hj5WZDKv8cu0",
    "outputId": "77743cb8-42b4-4c2c-ffbe-bfd20e321956"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of components-PCA</th>\n",
       "      <th>Number of components-MCA</th>\n",
       "      <th>Number of iterations</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Validation scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.055596e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.055596e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.055596e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.055596e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.055596e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48379</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>6.541368e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48380</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>6.581531e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48381</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>6.545761e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48382</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000e+09</td>\n",
       "      <td>6.521434e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48383</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>6.517862e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48384 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Number of components-PCA  Number of components-MCA  \\\n",
       "0                             3                         3   \n",
       "1                             3                         3   \n",
       "2                             3                         3   \n",
       "3                             3                         3   \n",
       "4                             3                         3   \n",
       "...                         ...                       ...   \n",
       "48379                        34                        10   \n",
       "48380                        34                        10   \n",
       "48381                        34                        10   \n",
       "48382                        34                        10   \n",
       "48383                        34                        10   \n",
       "\n",
       "       Number of iterations         Alpha  Validation scores  \n",
       "0                         1  1.000000e-10       1.055596e+09  \n",
       "1                         1  1.000000e-09       1.055596e+09  \n",
       "2                         1  1.000000e-08       1.055596e+09  \n",
       "3                         1  1.000000e-07       1.055596e+09  \n",
       "4                         1  1.000000e-06       1.055596e+09  \n",
       "...                     ...           ...                ...  \n",
       "48379                     9  1.000000e+06       6.541368e+09  \n",
       "48380                     9  1.000000e+07       6.581531e+09  \n",
       "48381                     9  1.000000e+08       6.545761e+09  \n",
       "48382                     9  1.000000e+09       6.521434e+09  \n",
       "48383                     9  1.000000e+10       6.517862e+09  \n",
       "\n",
       "[48384 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the results \n",
    "valid_score = pd.DataFrame(data=comp_pca)\n",
    "valid_score['Number of components-MCA'] = comp_mca\n",
    "valid_score['Number of iterations'] = iter\n",
    "valid_score['Alpha']=alpha\n",
    "valid_score['Validation scores'] = MSE_valid\n",
    "valid_score = valid_score.rename(columns = {0:'Number of components-PCA'})\n",
    "valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "zhjsFOTC4Zuc",
    "outputId": "ab3b3ed1-839e-4610-9732-ab84b1a048ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of components-PCA    3.200000e+01\n",
       "Number of components-MCA    1.000000e+01\n",
       "Number of iterations        9.000000e+00\n",
       "Alpha                       1.000000e-10\n",
       "Validation scores           8.174067e+08\n",
       "Name: 45339, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The hyperparameter set that produces the least validation error\n",
    "best_score = valid_score[valid_score['Validation scores'] == min(valid_score['Validation scores'])]\n",
    "best_score = best_score.iloc[0]\n",
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7ejiZ0_Z6Yi"
   },
   "source": [
    "From the above results we can see that the best score is given by 32 PCA components, 10 MCA components, 9 iterations for PCA and MCA and alpha values of 1 * e-10. Lets now use these results in our train_valid dataset to evaluate it on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9kf-VGJd8XD"
   },
   "source": [
    "\n",
    "#### 1.5 Train+vald and Test set <a class=\"anchor\" id=\"1.5\"></a>\n",
    "\n",
    "##### 1.5.1 Rescaling the data - train+valid and test set <a class=\"anchor\" id=\"1.5.1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNVu6UziDgSa"
   },
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "scaler = StandardScaler() # Instantiate\n",
    "scaler.fit(X_train_valid_num) # Fitting the data\n",
    "X_train_valid_numeric_S = pd.DataFrame(scaler.transform(X_train_valid_num)) # transforming the train+valid data\n",
    "X_test_numeric_S = pd.DataFrame(scaler.transform(X_test_num)) # transforming the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjuMyv2d8XG"
   },
   "source": [
    "\n",
    "##### 1.5.2 Perform PCA, MCA and Ridge regression on the train+valid using the best parameters and evaluate on the test set <a class=\"anchor\" id=\"1.5.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQau_y638cvF"
   },
   "outputs": [],
   "source": [
    "#Using the best hyperparameter set from above, fit and transform PCA on numerical training+validation data and fit and transform MCA on the categorical training+validation data. \n",
    "\n",
    "pca = prince.PCA(n_components=int(best_score['Number of components-PCA']),n_iter=int(best_score['Number of iterations']),random_state=42)\n",
    "pca = pca.fit(X_train_valid_numeric_S)\n",
    "X_train_valid_pca = pca.transform(X_train_valid_numeric_S)\n",
    "X_train_valid_pca = X_train_valid_pca.reset_index()\n",
    "\n",
    "mca = prince.MCA(n_components=int(best_score['Number of components-MCA']),n_iter=int(best_score['Number of iterations']),random_state=42)\n",
    "mca = mca.fit(X_train_valid_cat) \n",
    "X_train_valid_mca = mca.transform(X_train_valid_cat)\n",
    "X_train_valid_mca = X_train_valid_mca.reset_index()\n",
    "\n",
    "#Combine the results into one dataframe    \n",
    "X_train_valid_pmca = pd.concat([X_train_valid_pca,X_train_valid_mca],axis=1)\n",
    "\n",
    "# Fit ridge regression on the reduced features for the alpha value as identified above      \n",
    "lm = linear_model.Ridge(alpha = best_score['Alpha'])\n",
    "lm.fit(X_train_valid_pmca, y_train_valid)\n",
    "    \n",
    "#Transform the test-numerical and categorical data using PCA and MCA respectively    \n",
    "X_test_pca = pca.transform(X_test_numeric_S)\n",
    "X_test_pca = X_test_pca.reset_index()\n",
    "X_test_mca = mca.transform(X_test_cat)\n",
    "X_test_mca = X_test_mca.reset_index()\n",
    "    \n",
    "#Combine the PCA and MCA results from above into one dataframe  \n",
    "X_test_pmca = pd.concat([X_test_pca,X_test_mca],axis=1)\n",
    "\n",
    "#Evaluate the results on the test set and find the MSE   \n",
    "MSE_test = metrics.mean_squared_error(lm.predict(X_test_pmca),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oSrOL0_yHiJK",
    "outputId": "64c0f1ba-c614-448c-dc15-7c8a11dcc663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error on the test set is : 800322359.1532334\n"
     ]
    }
   ],
   "source": [
    "print(\"The prediction error on the test set is : {}\".format(MSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now compare the above results with regression on the original dataset i.e. without dimension reduction, using Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYNYLbjEd8XP"
   },
   "source": [
    "#### 1.6 Ridge regression on the original dataset <a class=\"anchor\" id=\"1.6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiwkKN0Hd8XY"
   },
   "outputs": [],
   "source": [
    "#Since we removed the variables with different levels, we will concat numerical and categorical from above to use the same features.\n",
    "X_train = pd.concat([X_train_num,X_train_cat], axis=1)\n",
    "X_valid = pd.concat([X_valid_num,X_valid_cat], axis=1)\n",
    "X_train_valid = pd.concat([X_train_valid_num,X_train_valid_cat], axis=1)\n",
    "X_test = pd.concat([X_test_num,X_test_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hwzQohN8d8Xa"
   },
   "outputs": [],
   "source": [
    "#Getting dummy variables for the categorical variables and dropping the first dummy variable\n",
    "X_train = pd.get_dummies(X_train,drop_first=True)\n",
    "X_valid = pd.get_dummies(X_valid,drop_first=True)\n",
    "X_train_valid = pd.get_dummies(X_train_valid,drop_first=True)\n",
    "X_test = pd.get_dummies(X_test,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ath_8rOGQeBt",
    "outputId": "ded1c995-843c-4625-aadf-3b921dd89dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the datasets: (1160, 59) (870, 59) (290, 59) (291, 59)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the datasets:', X_train_valid.shape, X_train.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msyCDMrYd8Xd"
   },
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "scaler = StandardScaler() # Instantiate\n",
    "scaler.fit(X_train) # Fitting the data\n",
    "X_train = pd.DataFrame(scaler.transform(X_train)) # transforming the training data\n",
    "X_valid = pd.DataFrame(scaler.transform(X_valid)) # transforming the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEvxkx8jd8Xf"
   },
   "outputs": [],
   "source": [
    "Validation_Scores = []\n",
    "# performing ridge regression on all the alpha values. Fit in the training data and predicting the validation set and finding MSE\n",
    "for a in alphas: #using the same alphas we defined above\n",
    "    lm_trainlasso = linear_model.Ridge(alpha=a)\n",
    "    lm_trainlasso.fit(X_train,y_train)\n",
    "    Validation_Scores.append(metrics.mean_squared_error(lm_trainlasso.predict(X_valid),y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGADcbbLd8Xj"
   },
   "outputs": [],
   "source": [
    "minerror_M1 = min(Validation_Scores) # min validation misclassification error\n",
    "besttrio_M1 = alphas[np.argmin(Validation_Scores)] #finding the alpha value that gives least error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "nO4HO9038cvp",
    "outputId": "7538e671-17a0-45b7-d6d1-936e5a23382a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alpha</th>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Error</th>\n",
       "      <td>7.869212e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Values\n",
       "Alpha             1.000000e-10\n",
       "Validation Error  7.869212e+08"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the results for least error\n",
    "bestparam = pd.DataFrame(index=['Alpha','Validation Error'],data=[besttrio_M1,minerror_M1])\n",
    "bestparam = bestparam.rename(columns = {0:'Values'})\n",
    "bestparam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha value is the same as above. We will now use this alpha value, fit our ridge regression and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9Fl9I9nd8Xo"
   },
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_valid)\n",
    "X_train_valid = pd.DataFrame(scaler.transform(X_train_valid))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test)) # transforming the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "cYAUwsPtd8Xq",
    "outputId": "0b39e3ab-3bdc-4efc-b961-b5bd510b1d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=Values    1.000000e-10\n",
       "Name: Alpha, dtype: float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refit model with train + validation set, perform prediction on test set\n",
    "lm1 = linear_model.Ridge(alpha = bestparam.loc['Alpha'])\n",
    "lm1.fit(X_train_valid, y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pyFSLopod8Xu",
    "outputId": "b598035c-1029-4cfe-ec18-3f6b5339b037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error for the test set is : 774598123.6178343\n"
     ]
    }
   ],
   "source": [
    "M1_terror = metrics.mean_squared_error(lm1.predict(X_test), y_test)\n",
    "print(\"The prediction error for the test set is : {}\".format(M1_terror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tm4ausmcp_x6"
   },
   "source": [
    "#### 1.7 Findings of Part I <a class=\"anchor\" id=\"1.7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "oVYsEXSDp-h1",
    "outputId": "c996605d-28c7-41d3-8a40-406b83b4c511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predition error on the test set- with PCA and MCA : 800322359.1532334\n",
      "Predition error on the test set- without PCA and MCA : 774598123.6178343\n"
     ]
    }
   ],
   "source": [
    "print(\"Predition error on the test set- with PCA and MCA : {}\".format(MSE_test))\n",
    "print(\"Predition error on the test set- without PCA and MCA : {}\".format(M1_terror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SJ3TkWWmrv9T"
   },
   "source": [
    "With all the other parameters being the same, the prediction error is lower when you don't do dimension reduction.\n",
    "The number of components chosen by PCA and MCA is the 42, whereas the total number of features in our dataset is 44. Not much of a difference there. Dimension reduction technique is particularly useful when you have hgih dimensional data. Since we had just 44 features, performing dimension reduction wasn't particularly useful. We could also use different dimension reduction techniques like Random forerst, Forward Feature Selection etc for different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atsOz7EuowQn"
   },
   "source": [
    "\n",
    "# **Part II : Clustering Analysis** <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjz44S1OdKPP"
   },
   "source": [
    "In the above section, we performed dimension reduction separately for numerical and categorical data. \n",
    "In this section, we would be performing clustering analysis on the same dataset using `Gower Distance` as a metric which is used to measure how different two records are by measuring numerical data and categorical data separately and then combines them to form a distance calculation. The distance is always a number between 0 (identical) and 1 (maximally dissimilar).\n",
    "Following are the steps performed for the clustering analysis:\n",
    "\n",
    "1. Creating an array `X_dist` to calculate the gower distance.\n",
    "2. Define the range of clusters that we want to create.\n",
    "3. Create random initial medoids equal to the number of clusters.\n",
    "4. Apply K-medoids using the gower distance matrix as input. K-Medoids uses existed points from input data space as medoids.\n",
    "5. Run the cluster analysis.\n",
    "6. Use the `get_clusters` and `get_medoids` function to determine the number of observations in each cluster and the medoids respectively.\n",
    "7. Define a for-loop to create a list `clustering_result` that records the cluster membership of each observation.\n",
    "8. Binning the response variable into the same number of categories as that of clusters.\n",
    "9. Computing and append the `Normalized Mutual Information` (a metric to measure clustering performance) between the clustering results and the binned categories. It normalizes the mutual information score to scale the results between 0 (no mutual information) and 1 (perfect correlation).\n",
    "10. Create a DataFrame to evaluate the `NMI` score with respect to each cluster value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgxWsiFid8X1"
   },
   "source": [
    "#### 2.1 Compute gower distance on the predictor set <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrtBvWFUowQ8"
   },
   "outputs": [],
   "source": [
    "# Load list of points for cluster analysis\n",
    "X_dist = gower.gower_matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P50ywXu2d8X6"
   },
   "source": [
    "#### 2.2 Apply K-medoids using the gower distance matrix as input<a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRkT1gYC49oD"
   },
   "outputs": [],
   "source": [
    "Correlation_score = [] # empty list to record NMI score\n",
    "Clusters = [] # empty list to record cluster value\n",
    "\n",
    "# Lets define the numbers of clusters that we want to create\n",
    "K = np.arange(1,11,1)\n",
    "\n",
    "# writing script to loop through different cluster values and evaluate NMI score for each value\n",
    "for val in K:\n",
    "\n",
    "    # Setting random initial medoids \n",
    "    center_index = np.random.randint(0, len(X_dist), val)\n",
    "\n",
    "    # Creating instance of K-Medoids algorithm\n",
    "    kmedoids_instance = kmedoids(X_dist, center_index, data_type='distance_matrix')\n",
    "\n",
    "    # Running cluster analysis to obtain results\n",
    "    kmedoids_instance.process()\n",
    "    clusters = kmedoids_instance.get_clusters()\n",
    "    #medoids = kmedoids_instance.get_medoids()\n",
    "\n",
    "    # creating a list 'y_pred' that records the cluster membership of each observation\n",
    "    clustering_result = [-1]*len(y.to_list()) # creating a list of same length as that of response variable\n",
    "    for c in range(val): # 'K' here is the number of clusters defined in the above code\n",
    "        for i in range(len(clusters[c])):\n",
    "            clustering_result[clusters[c][i]] = c\n",
    "\n",
    "    # Binning the response variable into the same number of categories as that of clusters which are created above. Here the value is 5.\n",
    "    ground_truth = pd.qcut(y, q=val, labels=range(val), precision=int)\n",
    "\n",
    "    # recording the cluster value\n",
    "    Clusters.append(val) \n",
    "\n",
    "    # Computing the normalized mutual information (NMI) between the clustering results and the binned categories\n",
    "    Correlation_score.append(normalized_mutual_info_score(ground_truth, clustering_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZclMMqPaowRA"
   },
   "source": [
    "#### 2.3 Comparing clustering results<a class=\"anchor\" id=\"2.3\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "zrfTvZStxaLj",
    "outputId": "21ae853e-4cd3-4ede-d06c-7a57d5272752"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clusters</th>\n",
       "      <th>NMI_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.394213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.276993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.236005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.243624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.224866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.195392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.181640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.191165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.185328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clusters  NMI_Score\n",
       "0         1   1.000000\n",
       "1         2   0.394213\n",
       "2         3   0.276993\n",
       "3         4   0.236005\n",
       "4         5   0.243624\n",
       "5         6   0.224866\n",
       "6         7   0.195392\n",
       "7         8   0.181640\n",
       "8         9   0.191165\n",
       "9        10   0.185328"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dataframe to evaluate the NMI score with respect to each cluster value\n",
    "score = pd.DataFrame(Clusters)\n",
    "score['NMI_Score'] = Correlation_score\n",
    "score = score.rename(columns = {0:'Clusters'})\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Findings of part II <a class=\"anchor\" id=\"2.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBW-d3reIQPz"
   },
   "source": [
    "From the above table, we get a perfect correlation between the ground truth and the clustering results when all the data is in one group. However, as the number of clusters increase, the NMI score decreases most of the time. This might be because the probabilities in NMI are approximated by frequency, resulting in finite size effect and preferring large number of partitions. This phenomenon is more evident in smaller networks. In this case when we increase the number of clusters then the class members are split across different clusters, the assignment is more in-complete, hence the NMI is decreasing. This could also be due to the small data size of just close to 1000 observations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_1_Divya_Syed_R5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
